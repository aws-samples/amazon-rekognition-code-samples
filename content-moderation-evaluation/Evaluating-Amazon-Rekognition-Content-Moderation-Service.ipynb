{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook for Evaluating Content Moderation Service\n",
    "\n",
    "This notebook is part of AWS blog series of Evaluating Content Moderation Service, which provide sample code to streamline steps from creating ground truth labeling job to generating evaluation metrics.\n",
    "\n",
    "Prerequisite: **You must prepare image dataset (evaluation dataset) ready for evaluation and [upload them to a S3 bucket](https://docs.aws.amazon.com/redshift/latest/dg/tutorial-loading-data-upload-files.html). The dataset should contain the moderation labels of interest to your use case.** \n",
    "\n",
    "The notebook has 3 major parts:\n",
    "1. Use Amazon SageMaker Ground Truth service to assign ground truth moderation labels to the evaluation dataset.\n",
    "2. Use Amazon Rekognition pretained moderation API to generate predicted labels for the evaluation dataset. \n",
    "3. Assess the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 0: Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get the latest installations of our dependencies. If your environment doesn't have AWS CLI installed, please follow [instructions here to set up AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install boto3 --upgrade\n",
    "!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start, it's necessary to create a bucket where to host evaluation dataset, then set proper values for following variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "BUCKET = '<YOUR S3 BUCKET NAME>'          # S3 bucket holds your evaluation dataset\n",
    "FILE_PREFIX = '<IMAGE PREFIX>'              # The prefix for your evaluation dataset\n",
    "EXP_NAME = '<JOB PREFIX>'                   # S3 prefix for SageMaker Ground Truth labeling job\n",
    "INPUT_MANIFEST = '<INPUT FILENAME>'         # Input manifest filename for SageMaker Ground Truth labeling job e.g. input.manifest\n",
    "OUTPUT_MANIFEST = '<OUTPUT FILENAME>'       # Output manifest filename for SageMaker Ground Truth labeling job e.g. output.manifest\n",
    "EVALUATION_FILE = '<DATASET FILENAME>'      # Filename to store evaluation dataset e.g. image.list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the bucket is in the same region as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.session.Session().region_name\n",
    "s3 = boto3.client(\"s3\")\n",
    "bucket_region = s3.head_bucket(Bucket=BUCKET)[\"ResponseMetadata\"][\"HTTPHeaders\"][\n",
    "    \"x-amz-bucket-region\"\n",
    "]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), \"You S3 bucket {} and this notebook need to be in the same region.\".format(BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Use Amazon SageMaker Ground Truth service to assign ground truth moderation labels to the evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create input manifest file for Ground Truth job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genenrate file that contains all images' name in evaluation dataset\n",
    "objects = s3.list_objects_v2(Bucket=BUCKET, Prefix=FILE_PREFIX)\n",
    "filenames = [o['Key'] for o in objects['Contents']]\n", 
    "\n",
    "if os.path.isfile(INPUT_MANIFEST):\n",
    "  os.remove(INPUT_MANIFEST)\n",
    "\n",
    "with open(INPUT_MANIFEST, 'w') as fp:\n",
    "    for filename in filenames:\n",
    "        if len(filename) != 0: \n",
    "            formatted_file = \"s3://{}/{}/{}\".format(BUCKET, FILE_PREFIX, filename)\n",
    "            fp.write('{\"source-ref\": \"' + formatted_file + '\"}\\n')\n",
    "            \n",
    "s3.upload_file(INPUT_MANIFEST, BUCKET, EXP_NAME + \"/\" + INPUT_MANIFEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Specify list of moderation labels for Ground Truth job\n",
    "To run an image classification labeling job, you need to decide on a set of classes the annotators can choose from. In our case, this list is [\"moderation_label_1\", \"moderation_label_2\", \"moderation_label_3\", \"moderation_label_4\", \"moderation_label_5\"]. In your own job you can choose any list of up to [service limit](https://docs.aws.amazon.com/sagemaker/latest/dg/input-data-limits.html#sms-label-quotas). We recommend the classes to be as unambiguous and concrete as possible. The categories should be mutually exclusive, For content moderation, you can reference [AWS Rekognition hierarchical taxonomy](https://docs.aws.amazon.com/rekognition/latest/dg/moderation.html#moderation-api) when creating those labels. In addition, be careful to make the task as objective as possible, unless of course your intention is to obtain subjective labels.\n",
    "\n",
    "To work with Ground Truth, this list needs to be converted to a .json file and uploaded to the S3 BUCKET\n",
    "\n",
    "_Note: The ordering of the labels or classes in the template governs the class indices that you will see downstream in the output manifest (this numbering is zero-indexed). In other words, the class that appears second in the template will correspond to class \"1\" in the output._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASS_LIST = [\"<moderation_label_1>\", \"<moderation_label_2\", \"<moderation_label_3\", \"<moderation_label_4\", \"<moderation_label_5\", \"Safe_Content\"]\n",
    "print(\"Label space is {}\".format(CLASS_LIST))\n",
    "\n",
    "json_body = {\"labels\": [{\"label\": label} for label in CLASS_LIST]}\n",
    "with open(\"class_labels.json\", \"w\") as f:\n",
    "    json.dump(json_body, f)\n",
    "\n",
    "s3.upload_file(\"class_labels.json\", BUCKET, EXP_NAME + \"/class_labels.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create instruction template for Ground Truth Workforce\n",
    "All of your evaluation dataset will be annotated by human annotators. It is critical to provide clear and concise instructions that help the annotators understand what you want to achieve. When used through the AWS Console, Ground Truth helps you create the instructions using a visual wizard. When using the API, you need to create an HTML template for your instructions. Below, we prepare a very simple but effective template and upload it to your S3 bucket.\n",
    "\n",
    "_NOTE:_\n",
    "- _If you use any images in your template (as we do), they need to be publicly accessible. You can enable public access to files in your S3 bucket through the S3 Console, as described in [3 Documentation](https://aws.amazon.com/premiumsupport/knowledge-center/read-access-objects-s3-bucket/). In this case, we need to have X+1 number of images, X equals to number of moderation labels you like to use plus \"safe content\" label. In our example, it's 6._ \n",
    "- _Replace placements with the actual image file names and moderation labels. e.g. image_1, image_2, ..., image_n, moderation_label_1, moderation_label_2, ..., moderation_label_n_\n",
    "- _The sample images image_1, image_2, etc., corresponds to moderation_label_1, moderation_label_2, etc., mentioned above, and image_6 should be an example of safe content._\n",
    "- _If you want to change number of moderation labels, you need to adjust sample images and labels accordingly._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing your template\n",
    "It is very easy to create broken template, This might not only cause your labeling job to fail but also cause your job to complete with meaningless results (when the annotators have no idea what to do, or the instructions are plain wrong). We highly recommend that you verify that your task is correct. The following cell creates and uploads a file called instructions.template to S3. It also creates instructions.html that you can open in a local browser window. Please do so and inspect the resulting web page; it should correspond to what you want your annotators to see (except the actual image to annotate will not be visible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the ground truth instruction template\n",
    "# make sure those objects are pubicly accessible in s3 bucket\n",
    "img_examples = [\n",
    "    \"https://s3.amazonaws.com/contentmoderation-accuracy-dataset/images/{}\".format(img_id)\n",
    "    for img_id in [\n",
    "        \"image_1\",\n",
    "        \"image_2\",\n",
    "        \"image_3\",\n",
    "        \"image_4\",\n",
    "        \"image_5\",\n",
    "        \"image_6\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "def make_template(test_template=False, save_fname=\"instructions.template\"):\n",
    "    template = r\"\"\"<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "    <crowd-form>\n",
    "      <crowd-image-classifier\n",
    "        name=\"crowd-image-classifier\"\n",
    "        src=\"{{{{ task.input.taskObject | grant_read_access }}}}\"\n",
    "        header=\"Dear Annotator, please tell me what you can see in the image. Thank you!\"\n",
    "        categories=\"{categories_str}\"\n",
    "      >\n",
    "        <full-instructions header=\"Image classification instructions\">\n",
    "        </full-instructions>\n",
    "\n",
    "        <short-instructions>\n",
    "          <p>Dear Annotator, please tell me whether what you can see in the image. Thank you!</p>\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"moderation_label_1\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"moderation_label_2\".</p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"moderation_label_3\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"moderation_label_4\". </p>\n",
    "\n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"moderation_label_5\". </p>\n",
    "          \n",
    "          <p><img src=\"{}\" style=\"max-width:100%\">\n",
    "          <br>Example \"Safe_Content\". </p>\n",
    "\n",
    "        </short-instructions>\n",
    "\n",
    "      </crowd-image-classifier>\n",
    "    </crowd-form>\"\"\".format(\n",
    "        *img_examples,\n",
    "        categories_str=str(CLASS_LIST)\n",
    "        if test_template\n",
    "        else \"{{ task.input.labels | to_json | escape }}\",\n",
    "    )\n",
    "\n",
    "    with open(save_fname, \"w\") as f:\n",
    "        f.write(template)\n",
    "    if test_template is False:\n",
    "        print(template)\n",
    "\n",
    "\n",
    "make_template(test_template=True, save_fname=\"instructions.html\")\n",
    "make_template(test_template=False, save_fname=\"instructions.template\")\n",
    "s3.upload_file(\"instructions.template\", BUCKET, EXP_NAME + \"/instructions.template\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define pre-built lambda functions for use in the labeling job\n",
    "Before we submit the job, we need to define the ARNs for key components of the labeling job: 1) the workteam, 2) the annotation consolidation Lambda function, 3) the pre-labeling task Lambda function, These functions are defined by strings with region names and AWS service account numbers, so we will define a mapping below that will enable you to run this notebook in corresponding AWS region (us-east-1 in our example).\n",
    "\n",
    "See the official documentation for the available ARNs:\n",
    "- Set **VERIFY_USING_PRIVATE_WORKFORCE=False** if you choose to use the [public workfofce](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management-public.html) or set **VERIFY_USING_PRIVATE_WORKFORCE=True** if you elect to use a [private workteam](https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-create-private-console.html) and check the corresponding ARN and set variable **private_workteam_arn**.\n",
    "- [Documentation](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_HumanTaskConfig.html#SageMaker-Type-HumanTaskConfig-PreHumanTaskLambdaArn) for available pre-human ARNs for other workflows.\n",
    "- [Documentation](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_AnnotationConsolidationConfig.html#SageMaker-Type-AnnotationConsolidationConfig-AnnotationConsolidationLambdaArn) for available annotation consolidation ANRs for other workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_workteam_arn = \"<ARN OF PRIVATE WORKFORCE>\"\n",
    "\n",
    "VERIFY_USING_PRIVATE_WORKFORCE = True\n",
    "\n",
    "# Specify ARNs for resources needed to run an image classification job.\n",
    "ac_arn_map = {\n",
    "    \"us-east-1\": \"432418664414\",\n",
    "}\n",
    "\n",
    "prehuman_arn = \"arn:aws:lambda:{}:{}:function:PRE-ImageMultiClass\".format(\n",
    "    region, ac_arn_map[region]\n",
    ")\n",
    "\n",
    "acs_arn = \"arn:aws:lambda:{}:{}:function:ACS-ImageMultiClass\".format(region, ac_arn_map[region])\n",
    "\n",
    "workteam_arn = \"arn:aws:sagemaker:{}:394669845002:workteam/public-crowd/default\".format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and submit the SageMaker Ground Truth job\n",
    "Make sure your [SageMaker execution role](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) has full access to [Amazon Cognito](https://aws.amazon.com/cognito/) as it is used as an identity provider to manager workforce permission in labeling task. The output manifest file is generated after GT job is complete. Mark the file name and location for later use. You can also adjust [labeling job parameters](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateLabelingJob.html#API_CreateLabelingJob_RequestParameters) to meet your specific business requirements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description = \"What do you see: a {}?\".format(\" a \".join(CLASS_LIST))\n",
    "task_keywords = [\"image\", \"classification\", \"humans\"]\n",
    "task_title = task_description\n",
    "job_name = \"ground-truth-cm-\" + str(int(time.time()))\n",
    "\n",
    "human_task_config = {\n",
    "    \"AnnotationConsolidationConfig\": {\n",
    "        \"AnnotationConsolidationLambdaArn\": acs_arn,\n",
    "    },\n",
    "    \"PreHumanTaskLambdaArn\": prehuman_arn,\n",
    "    \"MaxConcurrentTaskCount\": 200,               # 200 images will be sent at a time to the workteam.\n",
    "    \"NumberOfHumanWorkersPerDataObject\": 3,      # 3 separate workers will be required to label each image.\n",
    "    \"TaskAvailabilityLifetimeInSeconds\": 21600,  # Your worteam has 6 hours to complete all pending tasks.\n",
    "    \"TaskDescription\": task_description,\n",
    "    \"TaskKeywords\": task_keywords,\n",
    "    \"TaskTimeLimitInSeconds\": 60,                # Each image must be labeled within 1 minutes.\n",
    "    \"TaskTitle\": task_title,\n",
    "    \"UiConfig\": {\n",
    "        \"UiTemplateS3Uri\": \"s3://{}/{}/instructions.template\".format(BUCKET, EXP_NAME),\n",
    "    },\n",
    "}\n",
    "\n",
    "if not VERIFY_USING_PRIVATE_WORKFORCE:\n",
    "    human_task_config[\"PublicWorkforceTaskPrice\"] = {\n",
    "        \"AmountInUsd\": {\n",
    "            \"Dollars\": 0,\n",
    "            \"Cents\": 1,\n",
    "            \"TenthFractionsOfACent\": 2,\n",
    "        }\n",
    "    }\n",
    "    human_task_config[\"WorkteamArn\"] = workteam_arn\n",
    "else:\n",
    "    human_task_config[\"WorkteamArn\"] = private_workteam_arn\n",
    "\n",
    "ground_truth_request = {\n",
    "    \"InputConfig\": {\n",
    "        \"DataSource\": {\n",
    "            \"S3DataSource\": {\n",
    "                \"ManifestS3Uri\": \"s3://{}/{}/{}\".format(BUCKET, EXP_NAME, INPUT_MANIFEST),\n",
    "            }\n",
    "        },\n",
    "        \"DataAttributes\": {\n",
    "            \"ContentClassifiers\": [\"FreeOfPersonallyIdentifiableInformation\", \"FreeOfAdultContent\"]\n",
    "        },\n",
    "    },\n",
    "    \"OutputConfig\": {\n",
    "        \"S3OutputPath\": \"s3://{}/{}/output/\".format(BUCKET, EXP_NAME),\n",
    "    },\n",
    "    \"HumanTaskConfig\": human_task_config,\n",
    "    \"LabelingJobName\": job_name,\n",
    "    \"RoleArn\": role,\n",
    "    \"LabelAttributeName\": \"category\",\n",
    "    \"LabelCategoryConfigS3Uri\": \"s3://{}/{}/class_labels.json\".format(BUCKET, EXP_NAME),\n",
    "}\n",
    "\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "response = sagemaker_client.create_labeling_job(**ground_truth_request)\n",
    "labelingjob = response['LabelingJobArn'].split(\"/\")\n",
    "JOB_NAME = labelingjob[-1]\n",
    "\n",
    "OUTPUT_MANIFEST_LOCATION = \"s3://{}/{}/output/{}/manifests/output/{}\".format(BUCKET, EXP_NAME, JOB_NAME, OUTPUT_MANIFEST)\n",
    "print(JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Monitor job progress\n",
    "A Ground Truth job can take a few hours to complete depends on the number of images that need to be labeled. One way to monitor the job's progress is via AWS Console, or you can run the next cell repeatedly to check **LabelingJobStatus** value in Json response. Wait for labeling job successful completion on evaluation dataset and continue the next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_client.describe_labeling_job(LabelingJobName=JOB_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Use Amazon Rekognition pretained moderation API to generate predicted labels for the evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create function to generate predicted moderation labels on evaluation datasets using Amazon Rekognition moderation API. Optionally, you can adjust [MinConfidence](https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectModerationLabels.html#rekognition-DetectModerationLabels-request-MinConfidence) that Amazon Rekognition must have in order to return a moderated content label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client=boto3.client('rekognition')\n",
    "\n",
    "def moderate_image(photo, bucket):\n",
    "    response = client.detect_moderation_labels(Image={'S3Object':{'Bucket':bucket,'Name':photo}})\n",
    "    return len(response['ModerationLabels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Assess the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You first retrieve ground truth moderation labels from SageMaker Ground Truth labeling job results for evaluation dataset, then run Amazon Rekognition moderation API to get predicted moderation labels for the same dataset. Considering this is a binary classification problem (safe vs unsafe content), weâ€™re going to calculate following metrics (assuming unsafe content is positive):\n",
    "\n",
    "- [True Positive (TP)](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#true_positive)\n",
    "- [False Positive (FP)](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_negative_error)\n",
    "- [True Negative (TN)](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#true_negative)\n",
    "- [False Negative (FN)](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#False_negative_error)\n",
    "\n",
    "and corresponding evaluation metrics such as: \n",
    "\n",
    "- [False Positive Rate (FPR)](https://en.wikipedia.org/wiki/False_positive_rate)\n",
    "- [False Negative Rate (FNR)](https://en.wikipedia.org/wiki/False_positives_and_false_negatives#false_negative_rate)\n",
    "- [Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "- [Precision](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "Depends on the size of your evaluation dataset, this step will take some time to complete, keep monitoring the progress bar till \"Processing is complete\" message is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assume detected unsafe content is positive\n",
    "gt_exception_str='InternalServiceException'\n",
    "error_count=0\n",
    "safe_count=0\n",
    "unsafe_count=0\n",
    "gt_exception_count=0\n",
    "TP=0\n",
    "TN=0\n",
    "FP=0\n",
    "FN=0\n",
    "\n",
    "!aws s3 cp $OUTPUT_MANIFEST_LOCATION .\n",
    "\n",
    "f = open(OUTPUT_MANIFEST, \"r\")\n",
    "print('Processing is in progress')\n",
    "for x in f:\n",
    "    print('...')\n",
    "    info_list = x.split(\",\")\n",
    "    s3_filename='images/' + info_list[0].split(\"/\")[-1].replace(\"\\\"\",'')\n",
    "    gt_label=info_list[2].split(\":\")[-1].replace(\"\\\"\",'')\n",
    "    cm_label_count=moderate_image(s3_filename, BUCKET)\n",
    "    # print(gt_label)\n",
    "    if gt_label == \"Safe_Content\":\n",
    "        safe_count = safe_count + 1\n",
    "        if cm_label_count == 0:\n",
    "            TN = TN + 1\n",
    "        else:\n",
    "            FP = FP + 1\n",
    "    elif gt_exception_str in gt_label:\n",
    "        gt_exception_count = gt_exception_count + 1\n",
    "    else:\n",
    "        unsafe_count = unsafe_count + 1\n",
    "        if cm_label_count == 0:\n",
    "            FN = FN + 1\n",
    "        else:\n",
    "            TP = TP + 1\n",
    "\n",
    "print('Processing is complete')\n",
    "print(str(gt_exception_count) + \" GT tasks are failed\")\n",
    "print(\"TN is: \" + str(TN))\n",
    "print(\"FP is: \" + str(FP))\n",
    "print(\"FN is: \" + str(FN))\n",
    "print(\"TP is: \" + str(TP))\n",
    "\n",
    "# calculate evaluation metrics\n",
    "FPR = FP / (FP + TN)\n",
    "FNR = FN / (FN + TP)\n",
    "Recall = TP / (TP + FN)\n",
    "Precision = TP / (TP + FP)\n",
    "print(\"False Positive Rate is: \" + str(FPR))\n",
    "print(\"False Negative Rate is: \" + str(FNR))\n",
    "print(\"True Positive Rate(Recall) is: \" + str(Recall))\n",
    "print(\"Precision is: \" + str(Precision))"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
